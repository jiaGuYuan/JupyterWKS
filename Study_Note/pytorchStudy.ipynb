{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使每个Cell同时可输出多个语句的值\n",
    "%config ZMQInteractiveShell.ast_node_interactivity='all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy array\n",
    "np_data = np.arange(6).reshape((2, 3))\n",
    "np_data\n",
    "\n",
    "# numpy array --> torch tensor\n",
    "torch_data = torch.from_numpy(np_data)\n",
    "torch_data\n",
    "\n",
    "# torch tensor --> numpy array\n",
    "tensor2array = torch_data.numpy()\n",
    "tensor2array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用torch.cat拼接数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = torch.ones(100, 2)         # 数据的基本形态\n",
    "x0 = torch.normal(2*n_data, 1)      # x0.shape=(100, 2)\n",
    "x1 = torch.normal(-2*n_data, 1)     # x1.shape=(100, 2)\n",
    "\n",
    "# 注意 x, y 数据的数据形式是一定要像下面一样 (torch.cat 是在合并数据) x.shape=(200, 2)\n",
    "x = torch.cat((x0, x1), dim=0).type(torch.FloatTensor)  # FloatTensor = 32-bit floating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在新的维度长连接tensors序列. 输出的张量维度比输入的张量多一维\n",
    "stack(tensors, dim=0, out=None)\n",
    "   + tensors: 要连接的张量序列\n",
    "   + dim: 新增的维度(张量序列在这个维度上进行连接)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "t1 = torch.from_numpy(t1)\n",
    "t1\n",
    "\n",
    "t2 = np.array([[11, 12], [13, 14], [15, 16]])\n",
    "t2 = torch.from_numpy(t2)\n",
    "t2\n",
    "\n",
    "t1.shape\n",
    "t2.shape\n",
    "t = torch.stack((t1, t2), dim=0)\n",
    "t.shape\n",
    "t\n",
    "\n",
    "t = torch.stack([t1, t2], dim=1)\n",
    "t.shape\n",
    "t\n",
    "\n",
    "# dim最大只能取2, 因为t1/t2是2维的,stack增加了一个维度(3维) --对应dim=2\n",
    "t = torch.stack((t1, t2), dim=2)\n",
    "t.shape\n",
    "t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机生成数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从序列中随机选择\n",
    "从序列中随机选择(有放回)若干元素组成一个新序列. --np.random.choice\n",
    "+ eg: np.random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成指定维度的随机数\n",
    "\n",
    "+ eg: np.random.rand(3, 2)\n",
    "    * 生成指定维度的随机数,元素取值范围为[0, 1)\n",
    "    \n",
    "+ np.random.randint(0, 10, size=(3,2))\n",
    "    * 从指定区间范围中随机生成一个指定shape的array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomArrayFloat = np.random.rand(3, 2)  # 生成的是float64类型的\n",
    "randomArrayFloat\n",
    "randomArrayFloat.dtype\n",
    "randomArrayFloat = randomArrayFloat.astype('float32') # 转换成float32类型的\n",
    "\n",
    "# 从[0, 10)中随机生成一个数\n",
    "randomInt = np.random.randint(0, 10)\n",
    "randomInt\n",
    "\n",
    "# 从[0, 10)中随机生成一个指定shape的array\n",
    "randomIntArray = np.random.randint(0, 10, size=(3,2))\n",
    "randomIntArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按指定分布采样数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从均匀分布中抽取样本。-- np.random.uniform\n",
    "uniformData = np.random.uniform(1, 2, size=50) #从[1,2]中使用均匀分布抽取size个样本\n",
    "np.mean(uniformData)\n",
    "np.std(uniformData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据维度的说明"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "每个维度值说明该维度下子元素的个数.\n",
    "eg:\n",
    "ar =  np.array([[[[0., 1., 2., 3., 4.]],\n",
    "                 [[5., 6., 7., 8., 9.]]]])\n",
    "ar.shape ==> (1, 2, 1, 5)        \n",
    "关于上面输出维度的说明:\n",
    "[   --shape(1, 2, 1, 5)第一个维度为1,对应它有1个子元素\n",
    "    [  --shape(1, 2, 1, 5)第二个维度为2,对应它有2个子元素\n",
    "       [    --shape(1, 2, 1, 5)第三个维度为1,对应它只有1个子元素\n",
    "            [0., 1., 2., 3., 4.]  --shape(1, 2, 1, 5)最后一个维度为5,对应这里有5个元素\n",
    "       ],\n",
    "       [\n",
    "            [5., 6., 7., 8., 9.]\n",
    "       ]\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.linspace(0, 5, 5, dtype=np.float32, endpoint=False)\n",
    "data2 = np.linspace(5, 10, 5, dtype=np.float32, endpoint=False)\n",
    "data1 = np.row_stack((data1, data2))\n",
    "data1.shape\n",
    "\n",
    "data2 = data1[np.newaxis, :, np.newaxis, :]  # 新增了两个维度\n",
    "data2.shape\n",
    "data2\n",
    "\n",
    "'''\n",
    "输出:\n",
    "(2, 5)\n",
    "\n",
    "(1, 2, 1, 5)\n",
    "\n",
    "array([[[[0., 1., 2., 3., 4.]],\n",
    "\n",
    "        [[5., 6., 7., 8., 9.]]]], dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 做回归任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置超参数\n",
    "TIME_STEP = 10      # 每个batch中的样本数\n",
    "INPUT_SIZE = 1      # 输入RNN的单个样本的特征数\n",
    "LR = 0.02 # learning rate\n",
    "BIDIRECTIONAL= False #是否为双向RNN\n",
    "HIDDEN_SIZE = 32\n",
    "EPOCH = 100\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=HIDDEN_SIZE,     # rnn 单隐层神经元数目\n",
    "            num_layers=1,                # rnn 隐层数目\n",
    "            batch_first=True,            # input & output将batchsize放在第一个维度(batch, time_step, input_size) \n",
    "            bidirectional=BIDIRECTIONAL  #是否为双向RNN\n",
    "        )\n",
    "        in_features = HIDDEN_SIZE\n",
    "        if BIDIRECTIONAL: # 使用双向RNN时Linear的输入特征数要翻倍\n",
    "            in_features *= 2\n",
    "        self.out = nn.Linear(in_features, 1)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        # batch_first=True时各参数的shape\n",
    "        # x.shape (batch, time_step, input_size)\n",
    "        # h_state.shpae (n_layers * num_directions, batch, hidden_size) --双向RNN时num_directions为2,否则num_direction为1\n",
    "        # r_out.shpae (batch, time_step, hidden_size*num_directions)\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "\n",
    "        #print(r_out.shape)\n",
    "        outs = []    # 保存当前batch所有样本的预测输出\n",
    "        for time_step in range(r_out.size(1)):    # 计算当前batch每一个样本的预测输出\n",
    "            # r_out[:, time_step, :]获取的是每个样本的所有batch的最后一个隐层的参数\n",
    "            # self.out(r_out[:, time_step, :])则得到当前送入RNN的所有batch样本的预测值 shape=(batch, out_features)\n",
    "            outs.append(self.out(r_out[:, time_step, :])) \n",
    "        return torch.stack(outs, dim=1), h_state #torch.stack(outs, dim=1)得到shape=(batch, time_step, out_features)形状的结果作为下一个的输入\n",
    "\n",
    "\n",
    "    \n",
    "# 开始训练\n",
    "rnn = RNN()\n",
    "print(rnn)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # rnn参数优化器\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "plt.figure(1, figsize=(12, 5))\n",
    "plt.ion() # 为了能进行连续绘图\n",
    "\n",
    "h_state = None      # 初始化隐层状态\n",
    "for step in range(EPOCH):\n",
    "    # 为了演示一次向RNN送入多个batch数据的情况,手动构造两批样本点, 每次送入两个patch到RNN\n",
    "    start, end = step * np.pi, (step+1)*np.pi   \n",
    "    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32, endpoint=False)  # float32 for converting torch FloatTensor\n",
    "    x_np = np.sin(steps)\n",
    "    y_np = np.cos(steps)\n",
    "    x = torch.from_numpy(x_np[np.newaxis, :, np.newaxis])    # shape (batch, time_step, input_size)\n",
    "    y = torch.from_numpy(y_np[np.newaxis, :, np.newaxis])\n",
    "    \n",
    "    x_np2 = np.sin(steps)\n",
    "    y_np2 = (np.cos(steps) + np.random.rand(len(steps))/100).astype('float32') #加入扰动\n",
    "    x2 = torch.from_numpy(x_np2[np.newaxis, :, np.newaxis])\n",
    "    y2 = torch.from_numpy(y_np2[np.newaxis, :, np.newaxis])\n",
    "    \n",
    "    # 组合两个patch一次送入RNN\n",
    "    x = torch.cat((x, x2), dim=0)\n",
    "    y = torch.cat((y, y2), dim=0)\n",
    "\n",
    "    # 计算预测值, 计算误差, 进行误差反向传播\n",
    "    prediction, h_state = rnn(x, h_state)   # rnn output\n",
    "    h_state = h_state.data    # !! next step is important !! repack the hidden state, break the connection from last iteration\n",
    "    loss = loss_func(prediction, y)         # calculate loss\n",
    "    optimizer.zero_grad()                   # clear gradients for this training step\n",
    "    loss.backward()                         # backpropagation, compute gradients\n",
    "    optimizer.step()                        # apply gradients\n",
    "    \n",
    "\n",
    "    # plotting\n",
    "    plt.plot(steps, y_np.flatten(), 'r-')\n",
    "    np_prediction = prediction.data.numpy().flatten()\n",
    "    plt.plot(steps, (np_prediction[0:len(steps)]+np_prediction[len(steps):2*len(steps)])/2, 'b-')\n",
    "    plt.draw() \n",
    "    plt.pause(0.05)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
