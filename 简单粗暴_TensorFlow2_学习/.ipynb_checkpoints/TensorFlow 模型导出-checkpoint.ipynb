{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 SavedModel 完整导出模型\n",
    "\n",
    "\n",
    "在部署模型时，一般需要先将训练好的整个模型完整导出为一系列标准格式的文件，然后即可在不同的平台上部署模型文件。\n",
    "TensorFlow 为我们提供了 SavedModel 这一格式。与前面介绍的 Checkpoint 不同，SavedModel 包含了一个 TensorFlow 程序的完整信息： **不仅包含参数的权值，还包含计算的流程（即计算图）** 。\n",
    "当模型导出为 SavedModel 文件时，无需建立模型的源代码即可再次运行模型，这使得 SavedModel 尤其适用于模型的分享和部署。TensorFlow Serving（服务器端部署模型）、TensorFlow Lite（移动端部署模型）以及 TensorFlow.js 都会用到这一格式.\n",
    "\n",
    "\n",
    "由于SavedModel 基于计算图; 对于自定义的模型，要导出到SavedModel格式的方法需要使用 @tf.function 修饰,还要在修饰时指定 input_signature 参数，以显式说明输入的形状。\n",
    "```\n",
    "# 模型导出为 SavedModel\n",
    "tf.saved_model.save(model, \"保存的目标文件夹名称\")\n",
    "\n",
    "# 载入 SavedModel 文件时\n",
    "model = tf.saved_model.load(\"保存的目标文件夹名称\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "对于自定义的Keras模型(继承tf.keras.Model类)，使用SavedModel载入后将无法使用model()直接进行推断，而需要使用 model.call() 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Serving\n",
    "\n",
    "当我们将模型训练完毕后，往往需要将模型在生产环境中部署。最常见的方式，是在服务器上提供一个 API，即客户机向服务器的某个 API 发送特定格式的请求，服务器收到请求数据后通过模型进行计算，并返回结果。\n",
    "TensorFlow 为我们提供了 TensorFlow Serving 这一组件，能够帮助我们在实际生产环境中灵活且高性能地部署机器学习模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TensorFlow Serving 安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Serving 模型部署\n",
    "\n",
    "TensorFlow Serving 支持热更新模型，其典型的模型文件夹结构如下：\n",
    "```\n",
    "/saved_model_files\n",
    "    /1      # 版本号为1的模型文件\n",
    "        /assets\n",
    "        /variables\n",
    "        saved_model.pb\n",
    "    ...\n",
    "    /N      # 版本号为N的模型文件\n",
    "        /assets\n",
    "        /variables\n",
    "        saved_model.pb\n",
    "```\n",
    "    \n",
    "上面1~N的子文件夹代表不同版本号的模型。当指定 --model_base_path 时，只需要指定根目录的 绝对地址 （不是相对地址）即可。\n",
    "例如，如果上述文件夹结构存放在 home/snowkylin 文件夹内，则 --model_base_path 应当设置为 home/snowkylin/saved_model_files （不附带模型版本号）。TensorFlow Serving 会自动选择版本号最大的模型进行载入。\n",
    "\n",
    "\n",
    "**部署命令:**\n",
    "```\n",
    "tensorflow_model_server \\\n",
    "    --rest_api_port=8501 \\\n",
    "    --model_name=MLP \\\n",
    "    --model_base_path=\"/home/.../.../saved_with_signature\"  # 修改为自己模型的绝对地址\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义 Keras 模型的部署 \n",
    "\n",
    "使用继承 tf.keras.Model 类建立的自定义 Keras 模型的自由度相对更高。\n",
    "使用 TensorFlow Serving 部署模型时，对导出的 SavedModel 文件也有更多的要求：\n",
    "* 需要导出到 SavedModel 格式的方法（比如 call ）不仅需要使用 @tf.function 修饰，还要在修饰时指定 input_signature 参数，以显式说明输入的形状。\n",
    "该参数传入一个由 tf.TensorSpec 组成的列表，指定每个输入张量的形状和类型。例如，对于 MNIST 手写体数字识别，我们的输入是一个 [None, 28, 28, 1] 的四维张量 (None 表示第一维即 Batch Size 的大小不固定)，此时我们可以将模型的 call 方法做以下修饰：\n",
    "```\n",
    "class MLP(tf.keras.Model):\n",
    "    ...\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None, 28, 28, 1], tf.float32)])\n",
    "    def call(self, inputs):\n",
    "        ...\n",
    "```\n",
    "\n",
    "\n",
    "* 在将模型使用 tf.saved_model.save 导出时，需要通过signature参数提供待导出的函数的签名（Signature）。\n",
    "简单说来，由于自定义的模型类里可能有多个方法都需要导出，因此，需要告诉 TensorFlow Serving 每个方法在被客户端调用时分别叫做什么名字。\n",
    "例如，如果我们希望客户端在调用模型时使用 call 这一签名来调用 model.call 方法时，我们可以在导出时传入 signature 参数，以 dict 的键值对形式告知导出的方法对应的签名，代码如下：\n",
    "```\n",
    "model = MLP()\n",
    "...\n",
    "tf.saved_model.save(model, \"saved_with_signature/1\", signatures={\"call\": model.call})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在客户端调用以 TensorFlow Serving 部署的模型\n",
    "\n",
    "TensorFlow Serving 支持以 gRPC 和 RESTful API 调用以 TensorFlow Serving 部署的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
