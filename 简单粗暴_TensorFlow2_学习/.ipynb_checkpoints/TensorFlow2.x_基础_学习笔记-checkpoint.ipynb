{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考:https://tf.wiki/zh_hans/preface.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 包含以下特性：\n",
    "+ 训练流程\n",
    "    - 数据的处理 ：使用 tf.data 和 TFRecord 可以高效地构建和预处理数据集，构建训练数据流。同时可以使用 TensorFlow Datasets 快速载入常用的公开数据集。\n",
    "    - 模型的建立与调试 ：使用即时执行模式和著名的神经网络高层 API 框架 Keras，结合可视化工具 TensorBoard，简易、快速地建立和调试模型。也可以通过 TensorFlow Hub 方便地载入已有的成熟模型。\n",
    "    - 模型的训练 ：支持在 CPU、GPU、TPU 上训练模型，支持单机和多机集群并行训练模型，充分利用海量数据和计算资源进行高效训练。\n",
    "    - 模型的导出 ：将模型打包导出为统一的 SavedModel 格式，方便迁移和部署。\n",
    "\n",
    "+ 部署流程\n",
    "    - 服务器部署 ：使用 TensorFlow Serving 在服务器上为训练完成的模型提供高性能、支持并发、高吞吐量的 API。\n",
    "    - 移动端和嵌入式设备部署 ：使用 TensorFlow Lite 将模型转换为体积小、高效率的轻量化版本，并在移动端、嵌入式端等功耗和计算能力受限的设备上运行，支持使用 GPU 代理进行硬件加速，还可以配合 Edge TPU 等外接硬件加速运算。\n",
    "    - 网页端部署 ：使用 TensorFlow.js，在网页端等支持 JavaScript 运行的环境上也可以运行模型，支持使用 WebGL 进行硬件加速。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config ZMQInteractiveShell.ast_node_interactivity='all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[19 22]\n",
      " [43 50]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 环境测试\n",
    "tf.__version__\n",
    "A = tf.constant([[1, 2], [3, 4]])\n",
    "B = tf.constant([[5, 6], [7, 8]])\n",
    "C = tf.matmul(A, B) # 向量内积\n",
    "\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量 （Tensor）--数据的基本单位。TensorFlow 的张量在概念上等同于多维数组，我们可以使用它来描述数学中的标量（0 维数组）、向量（1 维数组）、矩阵（2 维数组）等各种量。\n",
    "\n",
    "```\n",
    "# shape=(N1, N2, ..., N_Dim) 表示数据有Dim个维度,并且指定的各维度分别有多少个子元素。\n",
    "# 标量 --0维\n",
    "scalar = tf.random.uniform(shape=())\n",
    "# 向量 --1维\n",
    "vector = tf.random.uniform(shape=(3,))\n",
    "# 矩阵 --2维\n",
    "matrix = tf.random.uniform(shape=(3, 2))\n",
    "print(scalar.shape)\n",
    "print(vector.shape)\n",
    "print(matrix.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个随机数（标量） --0维\n",
    "random_float = tf.random.uniform(shape=())\n",
    "\n",
    "# 定义一个有2个元素的零向量 --1维\n",
    "zero_vector = tf.zeros(shape=(2), dtype=tf.int32)\n",
    "\n",
    "# 定义两个2×2的常量矩阵 --2维\n",
    "A = tf.constant([[1., 2.], [3., 4.]])\n",
    "B = tf.constant([[5., 6.], [7., 8.]])\n",
    "\n",
    "# 查看形状、类型和值\n",
    "# print(random_float)\n",
    "# print(random_float.shape)\n",
    "# print(\"\\n\")\n",
    "# print(zero_vector)\n",
    "# print(zero_vector.shape)\n",
    "# print(\"\\n\")\n",
    "# print(A.shape)      # 输出(2, 2)，即矩阵的长和宽均为2\n",
    "# print(A.dtype)      # 输出<dtype: 'float32'>\n",
    "# print(A.numpy())    # 转换为NumPy数组\n",
    "\n",
    "C = tf.add(A, B)    # 计算矩阵A和B的和\n",
    "D = tf.matmul(A, B) # 计算矩阵A和B的乘积\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32) tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 自动求导机制 \n",
    "x = tf.Variable(initial_value=3.)\n",
    "with tf.GradientTape() as tape:     # 在 tf.GradientTape() 的上下文内，所有计算步骤都会被记录以用于求导\n",
    "    y = tf.square(x)                # y = x^2\n",
    "y_grad = tape.gradient(y, x)        # 计算y关于x的导数\n",
    "print(y, y_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 tf.Variable() 声明变量;变量与普通张量的一个重要区别是其默认能够被 TensorFlow 的自动求导机制所求导，因此往往被用于定义机器学习模型的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自动求偏导\n",
    "$L(w,b)=\\sum_{i=1}^{m}(x_iw+b-y_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(125.0, shape=(), dtype=float32) tf.Tensor(\n",
      "[[ 70.]\n",
      " [100.]], shape=(2, 1), dtype=float32) tf.Tensor(30.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 自动求偏导 \n",
    "X = tf.constant([[1., 2.], [3., 4.]])\n",
    "y = tf.constant([[1.], [2.]])\n",
    "w = tf.Variable(initial_value=[[1.], [2.]])  # w形状为(2x1),其导数形状也为(2x1)\n",
    "b = tf.Variable(initial_value=1.)\n",
    "with tf.GradientTape() as tape:\n",
    "    L = tf.reduce_sum(tf.square(tf.matmul(X, w) + b - y))\n",
    "w_grad, b_grad = tape.gradient(L, [w, b])        # 计算L(w, b)关于w, b的偏导数\n",
    "print(L, w_grad, b_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "年份: 2013, 2014, 2015, 2016, 2017\n",
    "房价: 12000, 14000, 15000, 16500, 17500\n",
    "通过线性模型y=ax+b来拟合上述数据.\n",
    "\n",
    "损失函数为: $L(a,b)=\\sum_{i=1}^{n}(ax_i+b-y_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763702027872223 0.057564988337455304\n"
     ]
    }
   ],
   "source": [
    "# 线性模型y=ax+b; 因为样本x的特征只有年份(即特征维度为0, 因此参数a为标量), b也为标量.\n",
    "# 求导较为方便。\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n",
    "\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n",
    "\n",
    "a, b = 0, 0\n",
    "\n",
    "num_epoch = 10000\n",
    "learning_rate = 5e-4\n",
    "for e in range(num_epoch):\n",
    "    # 手动计算损失函数关于自变量（模型参数）的梯度\n",
    "    y_pred = a * X + b\n",
    "    grad_a, grad_b = 2 * (y_pred - y).dot(X), 2 * (y_pred - y).sum()\n",
    "\n",
    "    # 更新参数\n",
    "    a, b = a - learning_rate * grad_a, b - learning_rate * grad_b\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考滤到y=wx+b; 当样本x的有多个特征时(即特征维度非0时, 因此参数w为向量), b为标量.\n",
    "如数据集如下时：\n",
    "X_raw = np.array([[2013], [2014], [2015], [2016], [2017]], dtype=np.float32)\n",
    "y_raw = np.array([[12000], [14000], [15000], [16500], [17500]], dtype=np.float32)\n",
    "\n",
    "假设函数：$\\hat{y}=ax+b$\n",
    "\n",
    "损失函数为: $L(w,b)=\\sum_{i=1}^{m}(wx_i+b-y_i)^2 = (wX+b-y)^2$\n",
    "\n",
    "L对w的偏导为： $\\frac{\\partial L(w,b)}{\\partial w} = 2*X^T(\\hat{y} - y)$  #不考滤对样本数m进行归一化\n",
    "\n",
    "L对b的偏导为： $\\frac{\\partial L(w,b)}{\\partial b} = 2*\\sum_{i=1}^{m}(\\hat{y} - y)$ #不考滤对样本数m进行归一化\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9763702]] 0.05756498831782566\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_raw = np.array([[2013], [2014], [2015], [2016], [2017]], dtype=np.float32)\n",
    "y_raw = np.array([[12000], [14000], [15000], [16500], [17500]], dtype=np.float32)\n",
    "\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n",
    "\n",
    "a, b = [[0]], 0\n",
    "\n",
    "num_epoch = 10000\n",
    "learning_rate = 5e-4\n",
    "for e in range(num_epoch):\n",
    "    # 手动计算损失函数关于自变量（模型参数）的梯度\n",
    "    y_pred = a * X + b\n",
    "    grad_a, grad_b = 2 * X.T.dot(y_pred - y), 2 * (y_pred - y).sum()\n",
    "\n",
    "    # 更新参数\n",
    "    a, b = a - learning_rate * grad_a, b - learning_rate * grad_b\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2013.],\n",
       "        [2014.],\n",
       "        [2015.],\n",
       "        [2016.],\n",
       "        [2017.]], dtype=float32),\n",
       " array([12000., 14000., 15000., 16500., 17500.], dtype=float32),\n",
       " array([[0.  ],\n",
       "        [0.25],\n",
       "        [0.5 ],\n",
       "        [0.75],\n",
       "        [1.  ]], dtype=float32),\n",
       " array([0.        , 0.36363637, 0.54545456, 0.8181818 , 1.        ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw = np.array([[2013], [2014], [2015], [2016], [2017]], dtype=np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n",
    "\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n",
    "X_raw, y_raw, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2013.],\n",
       "        [2014.],\n",
       "        [2015.],\n",
       "        [2016.],\n",
       "        [2017.]], dtype=float32),\n",
       " array([12000., 14000., 15000., 16500., 17500.], dtype=float32),\n",
       " array([[0.  ],\n",
       "        [0.25],\n",
       "        [0.5 ],\n",
       "        [0.75],\n",
       "        [1.  ]], dtype=float32),\n",
       " array([0.        , 0.36363637, 0.54545456, 0.8181818 , 1.        ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw = np.array([[2013], [2014], [2015], [2016], [2017]], dtype=np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n",
    "\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n",
    "X_raw, y_raw, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.25, 0.5 , 0.75, 1.  ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
